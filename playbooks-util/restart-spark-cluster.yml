# (Factored out to allow easy restarting for benchmarking tests)

- hosts: spark-master

  tasks:
  - name: "Stop Master"
    command: ./sbin/stop-master.sh
    args:
            chdir: /usr/local/spark/
    become: yes # needed to write log files?
    
  - name: "Start Master"
    command: ./sbin/start-master.sh
    args:
            chdir: /usr/local/spark/
    become: yes # needed to write log files?


- hosts: spark-nodes

  tasks:
  - name: "Stop slave"
    command: ./sbin/stop-slave.sh
    args:
            chdir: /usr/local/spark/
    become: yes # needed to write log files?



#  Options:
#    -c CORES, --cores CORES  Number of cores to use
#    -m MEM, --memory MEM     Amount of memory to use (e.g. 1000M, 2G)
#    -d DIR, --work-dir DIR   Directory to run apps in (default: SPARK_HOME/work)
#    -i HOST, --ip IP         Hostname to listen on (deprecated, please use --host or -h)
#    -h HOST, --host HOST     Hostname to listen on
#    -p PORT, --port PORT     Port to listen on (default: random)
#    --webui-port PORT        Port for web UI (default: 8081)
#    --properties-file FILE   Path to a custom Spark properties file.
#                             Default is conf/spark-defaults.conf.

 # TODO: get hostname from hosts?
  - name: "Start slave"
    vars:
      spark_master_host: "spark://spark-prod"
    command: "./sbin/start-slave.sh {{spark_master_host}}"
    args:
            chdir: /usr/local/spark/
    become: yes # needed to write log files?
